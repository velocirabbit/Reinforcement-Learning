{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Policy Gradient Agent (Cartpole Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# In this environment, an action receives a reward of +1 if it doesn't cause\n",
    "# the cartpole to fall, and a reward of -1 if it does. The episode is over\n",
    "# when the cartpole falls, or if `max_ep` actions are taken without it falling\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "total_episodes = 5000\n",
    "max_ep = 999\n",
    "update_frequency = 5  # Number of episodes to complete before updating network parameters\n",
    "gamma = 0.99  # reward discount factor\n",
    "h_size = 8  # size of the hidden layer in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The policy-based agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    # Take 1D float array of rewards and compute discounted reward\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add*gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    # Since the state/observation space is represented by a 4D bounding Box,\n",
    "    # `s_size = 4` for this example. Can check by doing: `print(env.observation_space)`\n",
    "    # `h_size` is the dimension of the hidden layer in the network\n",
    "    def __init__(self, lr, s_size, a_size, h_size):\n",
    "        # Establish the feed-forward part of the network\n",
    "        self.state_in = tf.placeholder(shape = [None, s_size], dtype = tf.float32)  # [None, s_size]\n",
    "        hidden = tf.contrib.layers.fully_connected(                                 # [None, h_size]\n",
    "            self.state_in, h_size, biases_initializer = None,\n",
    "            activation_fn = tf.nn.relu\n",
    "        )\n",
    "        self.output = tf.contrib.layers.fully_connected(                            # [None, a_size]\n",
    "            hidden, a_size, activation_fn = tf.nn.softmax,\n",
    "            biases_initializer = None\n",
    "        )\n",
    "        self.chosen_action = tf.argmax(self.output, 1)\n",
    "        \n",
    "        # Establish the training procedure. Feed the reward and chosen\n",
    "        # action into the network to compute the loss, using it to\n",
    "        # update the network\n",
    "        self.reward_holder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "        # Gets the index of the action for each row/time step in output\n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0])*tf.shape(self.output)[1] + self.action_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
    "        \n",
    "        tvars = tf.trainable_variables()  # returns all variables created with `trainable = True`\n",
    "        self.gradient_holders = []\n",
    "        for idx, var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32, name = str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        # Get gradients of each trainable variable wrt loss\n",
    "        self.gradients = tf.gradients(self.loss, tvars)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, tvars))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "myAgent = agent(lr = 1e-2, s_size = s_size, a_size = a_size, h_size = h_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 0)  Total reward = 19.000\n",
      "Ep. 100)  Total reward = 30.030\n",
      "Ep. 200)  Total reward = 34.130\n",
      "Ep. 300)  Total reward = 45.450\n",
      "Ep. 400)  Total reward = 62.470\n",
      "Ep. 500)  Total reward = 91.060\n",
      "Ep. 600)  Total reward = 103.680\n",
      "Ep. 700)  Total reward = 122.820\n",
      "Ep. 800)  Total reward = 154.970\n",
      "Ep. 900)  Total reward = 168.060\n",
      "Ep. 1000)  Total reward = 186.140\n",
      "Ep. 1100)  Total reward = 190.470\n",
      "Ep. 1200)  Total reward = 190.990\n",
      "Ep. 1300)  Total reward = 192.280\n",
      "Ep. 1400)  Total reward = 188.910\n",
      "Ep. 1500)  Total reward = 195.500\n",
      "Ep. 1600)  Total reward = 195.600\n",
      "Ep. 1700)  Total reward = 193.070\n",
      "Ep. 1800)  Total reward = 195.870\n",
      "Ep. 1900)  Total reward = 196.720\n",
      "Ep. 2000)  Total reward = 194.350\n",
      "Ep. 2100)  Total reward = 188.780\n",
      "Ep. 2200)  Total reward = 189.490\n",
      "Ep. 2300)  Total reward = 196.130\n",
      "Ep. 2400)  Total reward = 197.170\n",
      "Ep. 2500)  Total reward = 199.560\n",
      "Ep. 2600)  Total reward = 199.380\n",
      "Ep. 2700)  Total reward = 199.900\n",
      "Ep. 2800)  Total reward = 197.510\n",
      "Ep. 2900)  Total reward = 195.990\n",
      "Ep. 3000)  Total reward = 189.360\n",
      "Ep. 3100)  Total reward = 197.710\n",
      "Ep. 3200)  Total reward = 200.000\n",
      "Ep. 3300)  Total reward = 200.000\n",
      "Ep. 3400)  Total reward = 200.000\n",
      "Ep. 3500)  Total reward = 199.040\n",
      "Ep. 3600)  Total reward = 200.000\n",
      "Ep. 3700)  Total reward = 198.530\n",
      "Ep. 3800)  Total reward = 199.110\n",
      "Ep. 3900)  Total reward = 200.000\n",
      "Ep. 4000)  Total reward = 199.750\n",
      "Ep. 4100)  Total reward = 199.240\n",
      "Ep. 4200)  Total reward = 193.320\n",
      "Ep. 4300)  Total reward = 197.010\n",
      "Ep. 4400)  Total reward = 198.930\n",
      "Ep. 4500)  Total reward = 199.860\n",
      "Ep. 4600)  Total reward = 198.670\n",
      "Ep. 4700)  Total reward = 199.980\n",
      "Ep. 4800)  Total reward = 197.980\n",
      "Ep. 4900)  Total reward = 191.570\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    total_reward = []\n",
    "    total_length = []\n",
    "    # Zero out gradients of trainable variables\n",
    "    gradBuffer = sess.run(tf.trainable_variables())  # Get each trainable variables' gradient tensor\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad*0  \n",
    "    \n",
    "    while i < total_episodes:\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        # Replay buffer, consisting of tuples (s, a, r, s1), except they'll be\n",
    "        # stored as an array instead of a tuple so `ep_history` can be passed\n",
    "        # into numpy's Array initializer function\n",
    "        ep_history = []  # Oldest tuples will be first\n",
    "        for j in range(max_ep):\n",
    "            # Probabilistically pick an action given our network outputs\n",
    "            a_dist = sess.run(myAgent.output, feed_dict = {myAgent.state_in: [s]})\n",
    "            #a = np.random.choice(a_dist[0], p = a_dist[0])\n",
    "            #a = np.argmax(a_dist == a)\n",
    "            a = np.random.choice(a_dist.shape[1], p = a_dist[0])\n",
    "            \n",
    "            s1, r, d, _ = env.step(a)  # Get reward for taking action a given the bandit\n",
    "            ep_history.append([s, a, r, s1])\n",
    "            s = s1  # Update currently observed state\n",
    "            running_reward += r  # Update running total of rewards\n",
    "            if d:  # If the agent is done for this episode\n",
    "                # Update the network\n",
    "                ep_history = np.array(ep_history)\n",
    "                # Discount `r` for each row/time step in ep_history\n",
    "                ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
    "                feed_dict = {\n",
    "                    myAgent.reward_holder: ep_history[:,2],       # reward history\n",
    "                    myAgent.action_holder: ep_history[:,1],       # action history\n",
    "                    myAgent.state_in: np.vstack(ep_history[:,0])  # state history\n",
    "                }\n",
    "                grads = sess.run(myAgent.gradients, feed_dict = feed_dict)\n",
    "                for idx, grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad  # Accumulate gradients\n",
    "                \n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    # Use the accumulated gradients to update the network, then reset gradients\n",
    "                    feed_dict = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(myAgent.update_batch, feed_dict = feed_dict)\n",
    "                    for ix, grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad*0  # Zero out gradients\n",
    "                total_reward.append(running_reward)\n",
    "                total_length.append(j)\n",
    "                break\n",
    "        # Update our running tally of scores\n",
    "        if i % 100 == 0:\n",
    "            print(\"Ep. %d)  Total reward = %.3f\" % (i, np.mean(total_reward[-100:])))\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
