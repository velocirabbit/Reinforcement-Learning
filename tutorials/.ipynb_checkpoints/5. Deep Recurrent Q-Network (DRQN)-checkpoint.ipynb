{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Recurrent Q-Network (DRQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from gridworld import GameEnv  # The actual game environment\n",
    "from dqnHelper import *        # Helper functions for training DQNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment\n",
    "The game environment is the same as in the previous example: the agent (blue) moves around, trying to reach any of the goal spaces (green) while avoiding the fire spaces (red). Reaching either ends the episode, but reaching a goal space results in a +1 reward, while hitting a fire space results in a -1 reward. This time, we're only going to give the agent partial knowledge of its environment (a number of spaces around its current location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bdfaa61ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Environment parameters\n",
    "env_sz = 16  # Size of the game environment\n",
    "n_goals = 4  # Number of goal spaces\n",
    "n_fire = 16  # Number of fire spaces\n",
    "img_sz = 36  # Size of the output image fed to the agent\n",
    "view_sz = 4  # Vision radius\n",
    "\n",
    "env = GameEnv(\n",
    "    size = env_sz, n_goals = n_goals, n_fire = n_fire, partial = view_sz, img_sz = img_sz\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself\n",
    "This implementation is of a dueling DQN, and we'll be training it using a Double DQN setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, img_sz, h_size, rnn_cell, lr = 0.0001, scope = ''):\n",
    "        # The smallest image size allowed by this network is a 28x28 pixel image\n",
    "        num_pixels = img_sz**2 * 3\n",
    "        # Image input (as a flattened array)\n",
    "        self.scalar_in = tf.placeholder(shape = [None, num_pixels], dtype = tf.float32, name = '_'.join([scope, 'scalar_in']))\n",
    "        # Reshape input into a batch of 3D tensors with shape (img_sz, img_sz, 3)\n",
    "        self.img_in = tf.reshape(self.scalar_in, shape = [-1, img_sz, img_sz, 3])\n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv1 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.img_in, num_outputs = 32,\n",
    "            kernel_size = [8, 8], stride = [4, 4],\n",
    "            padding = 'valid', biases_initializer = None,\n",
    "            scope = '_'.join([scope, 'conv1'])\n",
    "        )\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv1, num_outputs = 64,\n",
    "            kernel_size = [4, 4], stride = [2, 2],\n",
    "            padding = 'valid', biases_initializer = None,\n",
    "            scope = '_'.join([scope, 'conv2'])\n",
    "        )\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv2, num_outputs = 128,\n",
    "            kernel_size = [2, 2], stride = [1, 1],\n",
    "            padding = 'valid', biases_initializer = None,\n",
    "            scope = '_'.join([scope, 'conv3'])\n",
    "        )\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv3, num_outputs = h_size,\n",
    "            kernel_size = self.conv3.shape[1:-1], stride = [1, 1],\n",
    "            padding = 'valid', biases_initializer = None,\n",
    "            scope = '_'.join([scope, 'conv4'])\n",
    "        )\n",
    "        self.conv_out = tf.contrib.layers.flatten(self.conv4)\n",
    "        # Need to reshape the output from (batch_sz*trace_ln, h_size) to (batch_sz, trace_ln, h_size)\n",
    "        self.trace_ln = tf.placeholder(dtype = tf.int32, name = '_'.join([scope, 'trace_ln']))\n",
    "        self.batch_sz = tf.placeholder(dtype = tf.int32, shape = [], name = '_'.join([scope, 'batch_sz']))\n",
    "        self.conv_flat = tf.reshape(self.conv_out, [self.batch_sz, self.trace_ln, h_size])\n",
    "        \n",
    "        # Feed the resized convolution output and the RNN hidden state into the RNN cell\n",
    "        self.rnn_s_in = rnn_cell.zero_state(self.batch_sz, tf.float32)\n",
    "        self.rnn, self.rnn_state = tf.nn.dynamic_rnn(\n",
    "            inputs = self.conv_flat, cell = rnn_cell, initial_state = self.rnn_s_in,\n",
    "            dtype = tf.float32, scope = '_'.join([scope, 'rnn'])\n",
    "        )\n",
    "        # Reshape the output of the rnn back to (batch_sz*trace_ln, h_size)\n",
    "        self.rnn = tf.reshape(self.rnn, shape = [-1, h_size])\n",
    "        \n",
    "        # Split the RNN output in half for the advantage and value streams\n",
    "        self.advStream, self.valStream = tf.split(self.rnn, 2, 1)\n",
    "        # Send the advantage and value vectors through separate dense layers\n",
    "        self.advWts = tf.Variable(tf.random_normal([h_size//2, env.actions]))\n",
    "        self.advantage = tf.matmul(self.advStream, self.advWts)\n",
    "        \n",
    "        self.valWts = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        self.value = tf.matmul(self.valStream, self.valWts)\n",
    "        \n",
    "        # Combine the advantage and value streams into a single Q-value: Q(s, a) = V(s) + A(a)\n",
    "        mean_adv = tf.reduce_mean(self.advantage, axis = 1, keepdims = True)\n",
    "        self.q_out = self.value + tf.subtract(self.advantage, mean_adv)\n",
    "        # Predict/suggest the next action\n",
    "        self.predict = tf.argmax(self.q_out, 1)\n",
    "        \n",
    "        # Single out the Q-value for the predicted action\n",
    "        self.actions = tf.placeholder(shape = [None], dtype = tf.int32, name = '_'.join([scope, 'actions']))\n",
    "        self.actions_ohv = tf.one_hot(self.actions, env.actions, dtype = tf.float32)\n",
    "        self.predict_q = tf.reduce_sum(tf.multiply(self.q_out, self.actions_ohv), axis = 1)\n",
    "        \n",
    "        # Compute the MSE loss between the predicted and target Q-values for the\n",
    "        # target action. The target Q-values should be obtained from the target DQN\n",
    "        self.targetQ = tf.placeholder(shape = [None], dtype = tf.float32, name = '_'.join([scope, 'targetQ']))\n",
    "        self.td_error = tf.square(self.targetQ - self.predict_q)\n",
    "        \n",
    "        # In order to only propagate accurate gradients through the network, we will mask\n",
    "        # the first half of the losses for each trace as per \"Playing FPS Games with Deep\n",
    "        # Reinforcement Learning\" (Lample & Chatlot, 2016)\n",
    "        self.mask_old = tf.zeros([self.batch_sz, self.trace_ln//2])\n",
    "        self.mask_new = tf.ones( [self.batch_sz, self.trace_ln//2])\n",
    "        self.mask = tf.reshape(tf.concat([self.mask_old, self.mask_new], 1), [-1])\n",
    "        self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "        \n",
    "        # TF ops to run parameter optimization gradients and updates\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "This class allows us to store experiences and sample them randomly to train the network. To correctly train the DRQN, we'll need to store entire episodes in the replay buffer (rather than storing steps in a single episode, clearing the buffer after each episode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer:\n",
    "    def __init__(self, buffer_size = 1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0 : len(self.buffer)-self.buffer_size+1] = []\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_sz, trace_ln):\n",
    "        sampled_eps = random.sample(self.buffer, batch_sz)\n",
    "        sampled_traces = []\n",
    "        for ep in sampled_eps:\n",
    "            try:\n",
    "                t_s = np.random.randint(0, len(ep)-trace_ln+1)\n",
    "            except:\n",
    "                print('len(self.buffer) = %d  | len(sampled_eps) = %d' % (len(self.buffer), len(sampled_eps)))\n",
    "                print('len(ep) = %d  | trace_ln = %d' % (len(ep), trace_ln))\n",
    "                print(self.buffer, '\\n\\n\\n')\n",
    "                print(sampled_eps)\n",
    "            t_e = t_s + trace_ln\n",
    "            sampled_traces.append(ep[t_s : t_e])\n",
    "        sampled_traces = np.array(sampled_traces)\n",
    "        return np.reshape(sampled_traces, [batch_sz*trace_ln, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "Training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 4             # How many experience traces to use for each training step\n",
    "trace_ln = 8             # How long each experience trace will be during training\n",
    "update_freq = 5          # How often to perform a training step\n",
    "gamma = 0.99             # Discount factor on the target Q-values\n",
    "e_start = 1              # Starting chance of random action\n",
    "e_end = 0.1              # Final chance of random action\n",
    "annealing_steps = 10000  # How many steps of traiing to reduce e_start to e_end\n",
    "num_episodes = 10000     # How many episodes of game environment to train network with\n",
    "pre_train_steps = 10000  # How many steps of random actions to take before training begins\n",
    "max_ep_ln = 50           # Max allowed length of an episode\n",
    "h_size = 512             # Hidden layer size (before advantage/value split)\n",
    "tau = 0.001              # Weight of main DQN's parameters to use when updating the target DQN: tau*mainDQN + (1-tau)*targetDQN\n",
    "lr = 0.0001              # Learning rate\n",
    "buffer_size = 1000       # Size of the experience replay buffer\n",
    "\n",
    "log_time = 100           # Number of episodes between log outputs\n",
    "ckpt_time = 1000         # Number of episodes between model checkpoints\n",
    "load_model = False\n",
    "save_dir = 'chkpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Initialize the RNN cells for each network\n",
    "main_rnncell   = tf.contrib.rnn.BasicLSTMCell(num_units = h_size, state_is_tuple = True)\n",
    "target_rnncell = tf.contrib.rnn.BasicLSTMCell(num_units = h_size, state_is_tuple = True)\n",
    "# Initialize the two dueling DQNs\n",
    "mainDQN   = QNetwork(img_sz, h_size, main_rnncell,   lr, 'main')\n",
    "targetDQN = QNetwork(img_sz, h_size, target_rnncell, lr, 'target')\n",
    "\n",
    "# Initialize variables and model saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Add the TF ops to update the targetDQN parameters to the graph\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables, tau)\n",
    "\n",
    "# Initialize the experience replay buffer\n",
    "replay_buffer = experience_buffer(buffer_size = buffer_size)\n",
    "\n",
    "# Set the rate decrease for the probability of a random action\n",
    "e = e_start\n",
    "step_drop = (e_start - e_end) / annealing_steps\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "# Total steps counter\n",
    "total_steps = 0\n",
    "\n",
    "# Saved model checkpoints directory\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual training algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep.   100)  Time: 20.26 sec\tSteps:   5000\tAvg. reward: -0.98\tP(random action) = 1.00\n",
      "Ep.   200)  Time: 20.05 sec\tSteps:  10000\tAvg. reward: -1.16\tP(random action) = 1.00\n",
      "Ep.   300)  Time: 190.87 sec\tSteps:  15000\tAvg. reward: -0.86\tP(random action) = 0.55\n",
      "Ep.   400)  Time: 177.05 sec\tSteps:  20000\tAvg. reward: -0.02\tP(random action) = 0.10\n",
      "Ep.   500)  Time: 178.81 sec\tSteps:  25000\tAvg. reward: 0.32\tP(random action) = 0.10\n",
      "Ep.   600)  Time: 186.53 sec\tSteps:  30000\tAvg. reward: 0.62\tP(random action) = 0.10\n",
      "Ep.   700)  Time: 185.21 sec\tSteps:  35000\tAvg. reward: 0.92\tP(random action) = 0.10\n",
      "Ep.   800)  Time: 184.73 sec\tSteps:  40000\tAvg. reward: 1.00\tP(random action) = 0.10\n",
      "Ep.   900)  Time: 191.43 sec\tSteps:  45000\tAvg. reward: 1.36\tP(random action) = 0.10\n",
      "Ep.  1000)  Time: 185.96 sec\tSteps:  50000\tAvg. reward: 1.55\tP(random action) = 0.10\n",
      "    -- >  Saved model checkpoint  <--\n",
      "Ep.  1100)  Time: 189.98 sec\tSteps:  55000\tAvg. reward: 1.79\tP(random action) = 0.10\n",
      "Ep.  1200)  Time: 194.58 sec\tSteps:  60000\tAvg. reward: 2.20\tP(random action) = 0.10\n",
      "Ep.  1300)  Time: 192.22 sec\tSteps:  65000\tAvg. reward: 2.22\tP(random action) = 0.10\n",
      "Ep.  1400)  Time: 188.58 sec\tSteps:  70000\tAvg. reward: 2.28\tP(random action) = 0.10\n",
      "Ep.  1500)  Time: 191.27 sec\tSteps:  75000\tAvg. reward: 2.43\tP(random action) = 0.10\n",
      "Ep.  1600)  Time: 197.24 sec\tSteps:  80000\tAvg. reward: 2.73\tP(random action) = 0.10\n",
      "Ep.  1700)  Time: 190.03 sec\tSteps:  85000\tAvg. reward: 1.93\tP(random action) = 0.10\n",
      "Ep.  1800)  Time: 195.42 sec\tSteps:  90000\tAvg. reward: 2.77\tP(random action) = 0.10\n",
      "Ep.  1900)  Time: 191.15 sec\tSteps:  95000\tAvg. reward: 2.60\tP(random action) = 0.10\n",
      "Ep.  2000)  Time: 191.69 sec\tSteps: 100000\tAvg. reward: 2.75\tP(random action) = 0.10\n",
      "    -- >  Saved model checkpoint  <--\n",
      "Ep.  2100)  Time: 195.16 sec\tSteps: 105000\tAvg. reward: 2.73\tP(random action) = 0.10\n",
      "Ep.  2200)  Time: 199.52 sec\tSteps: 110000\tAvg. reward: 2.94\tP(random action) = 0.10\n",
      "Ep.  2300)  Time: 189.43 sec\tSteps: 115000\tAvg. reward: 2.90\tP(random action) = 0.10\n",
      "Ep.  2400)  Time: 188.76 sec\tSteps: 120000\tAvg. reward: 3.42\tP(random action) = 0.10\n",
      "Ep.  2500)  Time: 192.12 sec\tSteps: 125000\tAvg. reward: 3.27\tP(random action) = 0.10\n",
      "Ep.  2600)  Time: 191.49 sec\tSteps: 130000\tAvg. reward: 2.69\tP(random action) = 0.10\n",
      "Ep.  2700)  Time: 190.13 sec\tSteps: 135000\tAvg. reward: 3.08\tP(random action) = 0.10\n",
      "Ep.  2800)  Time: 196.18 sec\tSteps: 140000\tAvg. reward: 3.20\tP(random action) = 0.10\n",
      "Ep.  2900)  Time: 190.24 sec\tSteps: 145000\tAvg. reward: 3.47\tP(random action) = 0.10\n",
      "Ep.  3000)  Time: 189.16 sec\tSteps: 150000\tAvg. reward: 3.39\tP(random action) = 0.10\n",
      "    -- >  Saved model checkpoint  <--\n",
      "Ep.  3100)  Time: 197.22 sec\tSteps: 155000\tAvg. reward: 3.17\tP(random action) = 0.10\n",
      "Ep.  3200)  Time: 188.19 sec\tSteps: 160000\tAvg. reward: 3.40\tP(random action) = 0.10\n",
      "Ep.  3300)  Time: 188.40 sec\tSteps: 165000\tAvg. reward: 3.00\tP(random action) = 0.10\n",
      "Ep.  3400)  Time: 192.37 sec\tSteps: 170000\tAvg. reward: 3.28\tP(random action) = 0.10\n",
      "Ep.  3500)  Time: 188.01 sec\tSteps: 175000\tAvg. reward: 3.34\tP(random action) = 0.10\n",
      "Ep.  3600)  Time: 188.73 sec\tSteps: 180000\tAvg. reward: 3.53\tP(random action) = 0.10\n",
      "Ep.  3700)  Time: 194.12 sec\tSteps: 185000\tAvg. reward: 3.64\tP(random action) = 0.10\n",
      "Ep.  3800)  Time: 187.86 sec\tSteps: 190000\tAvg. reward: 3.29\tP(random action) = 0.10\n",
      "Ep.  3900)  Time: 188.17 sec\tSteps: 195000\tAvg. reward: 3.33\tP(random action) = 0.10\n",
      "Ep.  4000)  Time: 193.82 sec\tSteps: 200000\tAvg. reward: 3.89\tP(random action) = 0.10\n",
      "    -- >  Saved model checkpoint  <--\n",
      "Ep.  4100)  Time: 190.72 sec\tSteps: 205000\tAvg. reward: 3.41\tP(random action) = 0.10\n",
      "Ep.  4200)  Time: 190.16 sec\tSteps: 210000\tAvg. reward: 2.94\tP(random action) = 0.10\n",
      "Ep.  4300)  Time: 200.53 sec\tSteps: 215000\tAvg. reward: 3.33\tP(random action) = 0.10\n",
      "Ep.  4400)  Time: 195.58 sec\tSteps: 220000\tAvg. reward: 3.34\tP(random action) = 0.10\n",
      "Ep.  4500)  Time: 188.68 sec\tSteps: 225000\tAvg. reward: 3.53\tP(random action) = 0.10\n",
      "Ep.  4600)  Time: 190.94 sec\tSteps: 230000\tAvg. reward: 3.63\tP(random action) = 0.10\n",
      "Ep.  4700)  Time: 191.75 sec\tSteps: 235000\tAvg. reward: 3.28\tP(random action) = 0.10\n",
      "Ep.  4800)  Time: 188.91 sec\tSteps: 240000\tAvg. reward: 3.53\tP(random action) = 0.10\n",
      "Ep.  4900)  Time: 191.55 sec\tSteps: 245000\tAvg. reward: 3.24\tP(random action) = 0.10\n",
      "Ep.  5000)  Time: 192.00 sec\tSteps: 250000\tAvg. reward: 3.86\tP(random action) = 0.10\n",
      "    -- >  Saved model checkpoint  <--\n",
      "Ep.  5100)  Time: 192.07 sec\tSteps: 255000\tAvg. reward: 4.21\tP(random action) = 0.10\n",
      "Ep.  5200)  Time: 190.42 sec\tSteps: 260000\tAvg. reward: 3.73\tP(random action) = 0.10\n",
      "Ep.  5300)  Time: 193.85 sec\tSteps: 265000\tAvg. reward: 3.65\tP(random action) = 0.10\n",
      "Ep.  5400)  Time: 187.76 sec\tSteps: 270000\tAvg. reward: 3.67\tP(random action) = 0.10\n",
      "Ep.  5500)  Time: 186.33 sec\tSteps: 275000\tAvg. reward: 3.62\tP(random action) = 0.10\n",
      "Ep.  5600)  Time: 193.29 sec\tSteps: 280000\tAvg. reward: 3.86\tP(random action) = 0.10\n",
      "Ep.  5700)  Time: 186.74 sec\tSteps: 285000\tAvg. reward: 3.46\tP(random action) = 0.10\n",
      "Ep.  5800)  Time: 185.79 sec\tSteps: 290000\tAvg. reward: 3.64\tP(random action) = 0.10\n",
      "Ep.  5900)  Time: 193.28 sec\tSteps: 295000\tAvg. reward: 3.39\tP(random action) = 0.10\n",
      "Ep.  6000)  Time: 187.70 sec\tSteps: 300000\tAvg. reward: 3.77\tP(random action) = 0.10\n",
      "    -- >  Saved model checkpoint  <--\n",
      "Ep.  6100)  Time: 192.64 sec\tSteps: 305000\tAvg. reward: 3.81\tP(random action) = 0.10\n",
      "Ep.  6200)  Time: 197.06 sec\tSteps: 310000\tAvg. reward: 4.05\tP(random action) = 0.10\n",
      "Ep.  6300)  Time: 189.22 sec\tSteps: 315000\tAvg. reward: 3.61\tP(random action) = 0.10\n",
      "Ep.  6400)  Time: 188.13 sec\tSteps: 320000\tAvg. reward: 4.07\tP(random action) = 0.10\n",
      "Ep.  6500)  Time: 191.97 sec\tSteps: 325000\tAvg. reward: 3.89\tP(random action) = 0.10\n",
      "Ep.  6600)  Time: 194.34 sec\tSteps: 330000\tAvg. reward: 3.85\tP(random action) = 0.10\n",
      "Ep.  6700)  Time: 190.67 sec\tSteps: 335000\tAvg. reward: 3.71\tP(random action) = 0.10\n",
      "Ep.  6800)  Time: 191.51 sec\tSteps: 340000\tAvg. reward: 3.75\tP(random action) = 0.10\n",
      "Ep.  6900)  Time: 193.77 sec\tSteps: 345000\tAvg. reward: 3.84\tP(random action) = 0.10\n",
      "Ep.  7000)  Time: 189.95 sec\tSteps: 350000\tAvg. reward: 3.75\tP(random action) = 0.10\n",
      "    -- >  Saved model checkpoint  <--\n",
      "Ep.  7100)  Time: 190.81 sec\tSteps: 355000\tAvg. reward: 3.74\tP(random action) = 0.10\n",
      "Ep.  7200)  Time: 196.78 sec\tSteps: 360000\tAvg. reward: 3.70\tP(random action) = 0.10\n",
      "Ep.  7300)  Time: 189.18 sec\tSteps: 365000\tAvg. reward: 3.64\tP(random action) = 0.10\n",
      "Ep.  7400)  Time: 187.22 sec\tSteps: 370000\tAvg. reward: 3.45\tP(random action) = 0.10\n",
      "Ep.  7500)  Time: 196.94 sec\tSteps: 375000\tAvg. reward: 3.69\tP(random action) = 0.10\n",
      "Ep.  7600)  Time: 189.48 sec\tSteps: 380000\tAvg. reward: 3.57\tP(random action) = 0.10\n",
      "Ep.  7700)  Time: 187.59 sec\tSteps: 385000\tAvg. reward: 3.80\tP(random action) = 0.10\n",
      "Ep.  7800)  Time: 195.36 sec\tSteps: 390000\tAvg. reward: 3.79\tP(random action) = 0.10\n",
      "Ep.  7900)  Time: 190.74 sec\tSteps: 395000\tAvg. reward: 4.34\tP(random action) = 0.10\n",
      "Ep.  8000)  Time: 189.17 sec\tSteps: 400000\tAvg. reward: 4.01\tP(random action) = 0.10\n",
      "    -- >  Saved model checkpoint  <--\n",
      "Ep.  8100)  Time: 194.03 sec\tSteps: 405000\tAvg. reward: 4.13\tP(random action) = 0.10\n",
      "Ep.  8200)  Time: 193.22 sec\tSteps: 410000\tAvg. reward: 4.01\tP(random action) = 0.10\n",
      "Ep.  8300)  Time: 186.01 sec\tSteps: 415000\tAvg. reward: 3.84\tP(random action) = 0.10\n",
      "Ep.  8400)  Time: 191.10 sec\tSteps: 420000\tAvg. reward: 3.69\tP(random action) = 0.10\n",
      "Ep.  8500)  Time: 192.01 sec\tSteps: 425000\tAvg. reward: 3.90\tP(random action) = 0.10\n",
      "Ep.  8600)  Time: 187.79 sec\tSteps: 430000\tAvg. reward: 3.56\tP(random action) = 0.10\n",
      "Ep.  8700)  Time: 189.41 sec\tSteps: 435000\tAvg. reward: 3.94\tP(random action) = 0.10\n",
      "Ep.  8800)  Time: 193.25 sec\tSteps: 440000\tAvg. reward: 3.84\tP(random action) = 0.10\n",
      "Ep.  8900)  Time: 187.35 sec\tSteps: 445000\tAvg. reward: 3.90\tP(random action) = 0.10\n",
      "Ep.  9000)  Time: 188.04 sec\tSteps: 450000\tAvg. reward: 3.72\tP(random action) = 0.10\n",
      "    -- >  Saved model checkpoint  <--\n",
      "Ep.  9100)  Time: 191.36 sec\tSteps: 455000\tAvg. reward: 4.31\tP(random action) = 0.10\n",
      "Ep.  9200)  Time: 192.68 sec\tSteps: 460000\tAvg. reward: 3.90\tP(random action) = 0.10\n",
      "Ep.  9300)  Time: 185.39 sec\tSteps: 465000\tAvg. reward: 3.99\tP(random action) = 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep.  9400)  Time: 196.08 sec\tSteps: 470000\tAvg. reward: 3.67\tP(random action) = 0.10\n",
      "Ep.  9500)  Time: 187.12 sec\tSteps: 475000\tAvg. reward: 4.12\tP(random action) = 0.10\n",
      "Ep.  9600)  Time: 185.65 sec\tSteps: 480000\tAvg. reward: 4.35\tP(random action) = 0.10\n",
      "Ep.  9700)  Time: 195.32 sec\tSteps: 485000\tAvg. reward: 3.75\tP(random action) = 0.10\n",
      "Ep.  9800)  Time: 187.44 sec\tSteps: 490000\tAvg. reward: 3.64\tP(random action) = 0.10\n",
      "Ep.  9900)  Time: 185.53 sec\tSteps: 495000\tAvg. reward: 3.52\tP(random action) = 0.10\n",
      "Ep. 10000)  Time: 192.86 sec\tSteps: 500000\tAvg. reward: 3.90\tP(random action) = 0.10\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if load_model:\n",
    "        print('Loading model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "    t_start = time.time()\n",
    "    \n",
    "    \n",
    "    # Main loop\n",
    "    for i in range(num_episodes):\n",
    "        ep_buffer = []\n",
    "        \n",
    "        # Reset the environment and get the first observation\n",
    "        s = processState(env.reset(), img_sz)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        \n",
    "        # Reset the RNN cell's hidden state\n",
    "        state = (np.zeros([1, h_size]), np.zeros([1, h_size]))\n",
    "        \n",
    "        # Training loop\n",
    "        while j < max_ep_ln:\n",
    "            j += 1\n",
    "            \n",
    "            # Choose an action by greedily (with e chance of a random acation) from mainDQN\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0, 4)\n",
    "                # Still need to get next RNN cell state\n",
    "                state1 = sess.run(mainDQN.rnn_state, feed_dict = {\n",
    "                    mainDQN.scalar_in: [s],\n",
    "                    mainDQN.rnn_s_in : state,\n",
    "                    mainDQN.batch_sz : 1,\n",
    "                    mainDQN.trace_ln : 1,\n",
    "                })\n",
    "            else:\n",
    "                a, state1 = sess.run([mainDQN.predict, mainDQN.rnn_state], feed_dict = {\n",
    "                    mainDQN.scalar_in   : [s],\n",
    "                    mainDQN.rnn_s_in : state,\n",
    "                    mainDQN.batch_sz : 1,\n",
    "                    mainDQN.trace_ln : 1,\n",
    "                })\n",
    "                a = a[0]\n",
    "            s1, r, d = env.step(a)\n",
    "            s1 = processState(s1, img_sz)\n",
    "            # Add this step to the episode's experience replay buffer\n",
    "            ep_buffer.append(np.reshape(np.array([s, a, r, s1, d]), [1, 5]))\n",
    "            total_steps += 1\n",
    "            if total_steps > pre_train_steps:  # Start training the DQNs\n",
    "                if e > e_end:\n",
    "                    e -= step_drop\n",
    "                \n",
    "                # If enough steps have passed, train the DQNs and update parameters\n",
    "                if total_steps % update_freq == 0:\n",
    "                    # Reset the RNN cell's hidden state\n",
    "                    state_train = (np.zeros([batch_sz, h_size]), np.zeros([batch_sz, h_size]))\n",
    "                    # Get a random sample of experiences to train with\n",
    "                    train_batch = replay_buffer.sample(batch_sz, trace_ln)\n",
    "                    # Run the two networks in feed-forward to get the predicted action\n",
    "                    # and the target Q-values\n",
    "                    a_predict = sess.run(mainDQN.predict, feed_dict = {\n",
    "                        mainDQN.scalar_in: np.vstack(train_batch[:,3]),\n",
    "                        mainDQN.rnn_s_in : state_train,\n",
    "                        mainDQN.batch_sz : batch_sz,\n",
    "                        mainDQN.trace_ln : trace_ln,\n",
    "                    })\n",
    "                    q_target = sess.run(targetDQN.q_out, feed_dict = {\n",
    "                        targetDQN.scalar_in: np.vstack(train_batch[:,3]),\n",
    "                        targetDQN.rnn_s_in : state_train,\n",
    "                        targetDQN.batch_sz : batch_sz,\n",
    "                        targetDQN.trace_ln : trace_ln,\n",
    "                    })\n",
    "                    # Zero out the Q-values for any steps that result in a finished episode\n",
    "                    end_multiplier = -(train_batch[:,4] - 1)\n",
    "                    # Get the Q-values of JUST the predicted actions from mainDQN\n",
    "                    doubleQ = q_target[range(batch_sz*trace_ln), a_predict]\n",
    "                    # Calculate the target Q-values. Note that steps with rewards (those that\n",
    "                    # result in the episode ending) have a Q-value of 0, while those without\n",
    "                    # a reward (steps that don't end the episode) do.\n",
    "                    # targetQ = r + gamma*Q(predicted_action)\n",
    "                    targetQ = train_batch[:,2] + (gamma*doubleQ * end_multiplier)\n",
    "                    # Update mainDQN with our targetQ values\n",
    "                    sess.run(mainDQN.updateModel, feed_dict = {\n",
    "                        mainDQN.scalar_in: np.vstack(train_batch[:,0]),\n",
    "                        mainDQN.targetQ  : targetQ,\n",
    "                        mainDQN.actions  : train_batch[:,1],\n",
    "                        mainDQN.rnn_s_in : state_train,\n",
    "                        mainDQN.batch_sz : batch_sz,\n",
    "                        mainDQN.trace_ln : trace_ln,\n",
    "                    })\n",
    "                    # Update targetDQN's parameters towards mainDQN's\n",
    "                    updateTarget(targetOps, sess)\n",
    "                    # End of model training step\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            state = state1\n",
    "            if d:\n",
    "                break\n",
    "            \n",
    "            # End of step\n",
    "        # Add the episode to the experience replay buffer\n",
    "        ep_buffer = list(zip(np.array(ep_buffer)))\n",
    "        replay_buffer.add(ep_buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        \n",
    "        # Periodically save the model\n",
    "        if i % ckpt_time == 0 and i != 0:\n",
    "            saver.save(sess, os.path.join(save_dir, 'model-'+str(i)+'.ckpt'))\n",
    "            print('    -- >  Saved model checkpoint  <--')\n",
    "        if len(rList) % log_time == 0 and len(rList) != 0:\n",
    "            t_elapsed = time.time() - t_start\n",
    "            print('Ep. {:5d})  Time: {:.2f} sec\\tSteps: {:6d}\\tAvg. reward: {:.2f}\\tP(random action) = {:.2f}'.format(\n",
    "                i+1, t_elapsed, total_steps, np.mean(rList[-log_time:]), e\n",
    "            ))\n",
    "            t_start = time.time()\n",
    "        # End of episode\n",
    "    # Save trained model\n",
    "    saver.save(sess, os.path.join(save_dir, 'model-'+str(i)+'ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check network learning\n",
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bd995f39b0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEL4GQl2IYTwM/8f5KKYCJZykIAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bdfadda438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList), [len(rList)//100, 100])\n",
    "rMean = np.average(rMat, 1)\n",
    "plt.plot(rMean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
