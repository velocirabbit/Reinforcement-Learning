{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bandits\n",
    "Here, we define our bandits. For this example, we are using a four-armed bandit. The pullBandit function generates a random number from a normal distribution with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned, We want our agent to learn to always choose the bandit that will give that positive reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out our bandits. Currently, bandit 4 is set to most often provide a positive reward\n",
    "bandits = [0.2, 0, -0.2, -5]\n",
    "num_bandits = len(bandits)\n",
    "def pullBandit(bandit):\n",
    "    # Get a random number\n",
    "    result = np.random.randn(1)\n",
    "    return 1 if result > bandit else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent\n",
    "The code below establishes our simple neural agent. It consists of a set of values for each of the bandits. Each value is an estimate of the value of the return from choosing the bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the received reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Establish the feed-forward part of the network that does the actual choosing\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "# Establish the training procedure, feeding the reward and chosen action into\n",
    "# the network to compute the loss, using the loss to update the network\n",
    "reward_holder = tf.placeholder(shape = [1], dtype = tf.float32)\n",
    "action_holder = tf.placeholder(shape = [1], dtype = tf.int32)\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "loss = -(tf.log(responsible_weight) * reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "We will train our agent by taking actions in our environment and receiving rewards. Using the rewards and actions, we can know how to properly update our network in order to more often choose actions that will yield the highest rewards over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the 4 bandits: [1. 0. 0. 0.]\n",
      "Running reward for the 4 bandits: [-1. -1.  1. 14.]\n",
      "Running reward for the 4 bandits: [-1. -1.  0. 61.]\n",
      "Running reward for the 4 bandits: [ -2.   1.  -1. 105.]\n",
      "Running reward for the 4 bandits: [ -1.   0.  -1. 151.]\n",
      "Running reward for the 4 bandits: [ -1.  -1.   1. 198.]\n",
      "Running reward for the 4 bandits: [  0.  -2.   1. 244.]\n",
      "Running reward for the 4 bandits: [  0.  -1.   2. 288.]\n",
      "Running reward for the 4 bandits: [  1.  -1.   3. 330.]\n",
      "Running reward for the 4 bandits: [  0.  -2.   3. 378.]\n",
      "Running reward for the 4 bandits: [ -4.  -2.   3. 424.]\n",
      "Running reward for the 4 bandits: [ -4.  -1.   3. 469.]\n",
      "Running reward for the 4 bandits: [ -4.  -2.   3. 518.]\n",
      "Running reward for the 4 bandits: [ -5.  -2.   3. 561.]\n",
      "Running reward for the 4 bandits: [ -5.  -4.   3. 609.]\n",
      "Running reward for the 4 bandits: [ -6.  -5.   4. 656.]\n",
      "Running reward for the 4 bandits: [ -6.  -5.   3. 703.]\n",
      "Running reward for the 4 bandits: [ -6.  -5.   3. 747.]\n",
      "Running reward for the 4 bandits: [ -6.  -6.   3. 794.]\n",
      "Running reward for the 4 bandits: [ -5.  -5.   3. 842.]\n",
      "\n",
      "\n",
      "The agent thinks bandit 4 is the most promising...\n",
      "... and it was right!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000\n",
    "total_reward = np.zeros(num_bandits)  # Set scorecard for bandits to 0\n",
    "e = 0.1  # Chance of taking a random action\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the TensorFlow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        # Choose either a random action or one from our network\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        # Get our reward from picking one of the bandits\n",
    "        reward = pullBandit(bandits[action])\n",
    "        # Update the network\n",
    "        _, resp, ww = sess.run(\n",
    "            [update, responsible_weight, weights],\n",
    "            feed_dict = {\n",
    "                reward_holder: [reward],\n",
    "                action_holder: [action]\n",
    "            }\n",
    "        )\n",
    "        # Update our running tally of scores\n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print(\"Running reward for the %d bandits: %s\" % (num_bandits, str(total_reward)))\n",
    "        i += 1\n",
    "print(\"\\n\\nThe agent thinks bandit %d is the most promising...\" % (np.argmax(ww)+1))\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "    print(\"... and it was right!\")\n",
    "else:\n",
    "    print(\"... and it was wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
