{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks and Beyond\n",
    "Practice implementing a DQN using both Double DQN and Dueling DQN. The agent learns to solve a navigation task in a basic grid world.\n",
    "\n",
    "Note to self (2D convolutional layer in TensorFlow):  \n",
    "`tf.contrib.layers.convolution2d(inputs, num_outputs, kernel_size, stride, padding)`  \n",
    "where:  \n",
    "* `num_outputs`: number of output filters  \n",
    "* `kernel_size`: size of convolution window  \n",
    "* `padding`: type of padding to apply (`\"valid\"`, `\"same\"`)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from gridworld import GameEnv  # The game environment\n",
    "from helper import *           # Helper functions for training DQNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment\n",
    "An example of the game environment below. The agent controls the blue square, and can move up, down, left, or right. The goal is to move to a green square (for a +1 reward) and avoid the red squares (for a -1 reward). The position of the blocks are randomized every episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sz = 7\n",
    "img_sz = 64\n",
    "n_pxls = img_sz**2 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x253bf50f160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = GameEnv(size = grid_sz, n_goals = 2, n_fire = 5, partial = False, img_sz = img_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself\n",
    "This implementation is of a dueling DQN, and we'll be training it using a double DQN setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, h_size, lr = 0.0001):\n",
    "        # The network receives a frame from the game, flattened into an array\n",
    "        # It then resizes it and processes it through four convolutional layers\n",
    "        # The smallest image size allowed by this network is a 28x28 pixel image\n",
    "        self.scalarInput = tf.placeholder(shape = [None, n_pxls], dtype = tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape = [-1, img_sz, img_sz, 3])\n",
    "        # Convolution layers\n",
    "        self.conv1 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.imageIn, num_outputs = 32,\n",
    "            kernel_size = [8, 8], stride = [4, 4],\n",
    "            padding = 'valid', biases_initializer = None\n",
    "        )\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv1, num_outputs = 64,\n",
    "            kernel_size = [4, 4], stride = [2, 2],\n",
    "            padding = 'valid', biases_initializer = None\n",
    "        )\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv2, num_outputs = 128,\n",
    "            kernel_size = [2, 2], stride = [1, 1],\n",
    "            padding = 'valid', biases_initializer = None\n",
    "        )\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(\n",
    "            inputs = self.conv3, num_outputs = h_size,\n",
    "            kernel_size = self.conv3.shape[1:-1], stride = [1, 1],\n",
    "            padding = 'valid', biases_initializer = None\n",
    "        )\n",
    "        # Take the output from the final convolutional layer and split it into\n",
    "        # two separate advantage and value streams\n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3)\n",
    "        self.streamAdv = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamVal = tf.contrib.layers.flatten(self.streamVC)\n",
    "        # Initialize weights\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.AdvWts = tf.Variable(xavier_init([h_size//2, env.actions]))\n",
    "        self.ValWts = tf.Variable(xavier_init([h_size//2, 1]))\n",
    "        self.Advantage = tf.matmul(self.streamAdv, self.AdvWts)\n",
    "        self.Value = tf.matmul(self.streamVal, self.ValWts)\n",
    "        \n",
    "        # Combine the advantage and value streams together to get our final Q-values\n",
    "        mean_adv = tf.reduce_mean(self.Advantage, axis = 1, keepdims = True)\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage, mean_adv)\n",
    "        self.predict = tf.argmax(self.Qout, 1)  # Outputs index of the action to take\n",
    "        \n",
    "        # Single out the predicted Q-value for just the predicted action by setting\n",
    "        # the other Q-values to 0\n",
    "        self.actions = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.actions, dtype = tf.float32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis = 1)\n",
    "        \n",
    "        # Obtain the loss by taking the sum of squares difference between the target\n",
    "        # and predicted Q-values. The target Q-values should be obtained from the\n",
    "        # other DQN (`targetDQN`). We don't compute the loss/error for the Q-values\n",
    "        # of the other actions\n",
    "        self.targetQ = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "This class allows us to store experiences and sample them randomly to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer:\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0 : len(experience)+len(self.buffer)-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)), [size, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "Training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32           # Number of experiences to use for each training step\n",
    "update_freq = 4           # How often to perform a training step\n",
    "y = 0.99                  # Discount factor on the target Q-values\n",
    "startE = 1                # Starting chance of a random action\n",
    "endE = 0.1                # Final chance of a random action\n",
    "annealing_steps = 10000   # How many steps of training to reduce startE to endE\n",
    "num_episodes = 10000      # How many episodes of game environment to train network with\n",
    "pre_train_steps = 10000   # How many steps of random actions before training begins\n",
    "max_epLength = 50         # Max allowed length of our episode\n",
    "h_size = 512              # Hidden layer size (before split into advantage/value streams)\n",
    "tau = 0.001               # Rate to update target network toward primary network\n",
    "lr = 0.0001               # Learning rate\n",
    "buffer_size = 50000       # Size of the experience replay buffer\n",
    "\n",
    "log_time = 100            # Number of episodes between log outputs\n",
    "load_model = False\n",
    "save_dir = 'chkpts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint\n",
      "Ep.  100)  Time: 8.95 sec\tSteps:   5000\t Avg. reward: -1.69\tP(random action) = 1.00\n",
      "Ep.  200)  Time: 7.84 sec\tSteps:  10000\t Avg. reward: -1.28\tP(random action) = 1.00\n",
      "Ep.  300)  Time: 207.08 sec\tSteps:  15000\t Avg. reward: -1.09\tP(random action) = 0.55\n",
      "Ep.  400)  Time: 216.43 sec\tSteps:  20000\t Avg. reward: 0.54\tP(random action) = 0.10\n",
      "Ep.  500)  Time: 216.08 sec\tSteps:  25000\t Avg. reward: 1.82\tP(random action) = 0.10\n",
      "Ep.  600)  Time: 219.49 sec\tSteps:  30000\t Avg. reward: 3.61\tP(random action) = 0.10\n",
      "Ep.  700)  Time: 221.80 sec\tSteps:  35000\t Avg. reward: 6.19\tP(random action) = 0.10\n",
      "Ep.  800)  Time: 221.62 sec\tSteps:  40000\t Avg. reward: 8.21\tP(random action) = 0.10\n",
      "Ep.  900)  Time: 232.37 sec\tSteps:  45000\t Avg. reward: 8.10\tP(random action) = 0.10\n",
      "Ep. 1000)  Time: 224.84 sec\tSteps:  50000\t Avg. reward: 8.93\tP(random action) = 0.10\n",
      "Saved model checkpoint\n",
      "Ep. 1100)  Time: 229.57 sec\tSteps:  55000\t Avg. reward: 10.32\tP(random action) = 0.10\n",
      "Ep. 1200)  Time: 238.14 sec\tSteps:  60000\t Avg. reward: 10.02\tP(random action) = 0.10\n",
      "Ep. 1300)  Time: 239.95 sec\tSteps:  65000\t Avg. reward: 9.84\tP(random action) = 0.10\n",
      "Ep. 1400)  Time: 230.21 sec\tSteps:  70000\t Avg. reward: 9.99\tP(random action) = 0.10\n",
      "Ep. 1500)  Time: 236.31 sec\tSteps:  75000\t Avg. reward: 10.14\tP(random action) = 0.10\n",
      "Ep. 1600)  Time: 228.85 sec\tSteps:  80000\t Avg. reward: 9.90\tP(random action) = 0.10\n",
      "Ep. 1700)  Time: 236.84 sec\tSteps:  85000\t Avg. reward: 10.26\tP(random action) = 0.10\n",
      "Ep. 1800)  Time: 232.21 sec\tSteps:  90000\t Avg. reward: 9.43\tP(random action) = 0.10\n",
      "Ep. 1900)  Time: 234.51 sec\tSteps:  95000\t Avg. reward: 9.58\tP(random action) = 0.10\n",
      "Ep. 2000)  Time: 238.14 sec\tSteps: 100000\t Avg. reward: 9.48\tP(random action) = 0.10\n",
      "Saved model checkpoint\n",
      "Ep. 2100)  Time: 234.22 sec\tSteps: 105000\t Avg. reward: 10.18\tP(random action) = 0.10\n",
      "Ep. 2200)  Time: 237.85 sec\tSteps: 110000\t Avg. reward: 9.23\tP(random action) = 0.10\n",
      "Ep. 2300)  Time: 237.82 sec\tSteps: 115000\t Avg. reward: 9.15\tP(random action) = 0.10\n",
      "Ep. 2400)  Time: 234.31 sec\tSteps: 120000\t Avg. reward: 9.51\tP(random action) = 0.10\n",
      "Ep. 2500)  Time: 240.16 sec\tSteps: 125000\t Avg. reward: 9.28\tP(random action) = 0.10\n",
      "Ep. 2600)  Time: 232.41 sec\tSteps: 130000\t Avg. reward: 9.80\tP(random action) = 0.10\n",
      "Ep. 2700)  Time: 237.41 sec\tSteps: 135000\t Avg. reward: 8.86\tP(random action) = 0.10\n",
      "Ep. 2800)  Time: 233.81 sec\tSteps: 140000\t Avg. reward: 9.18\tP(random action) = 0.10\n",
      "Ep. 2900)  Time: 232.00 sec\tSteps: 145000\t Avg. reward: 8.50\tP(random action) = 0.10\n",
      "Ep. 3000)  Time: 237.81 sec\tSteps: 150000\t Avg. reward: 8.50\tP(random action) = 0.10\n",
      "Saved model checkpoint\n",
      "Ep. 3100)  Time: 232.09 sec\tSteps: 155000\t Avg. reward: 9.14\tP(random action) = 0.10\n",
      "Ep. 3200)  Time: 233.99 sec\tSteps: 160000\t Avg. reward: 8.93\tP(random action) = 0.10\n",
      "Ep. 3300)  Time: 236.92 sec\tSteps: 165000\t Avg. reward: 7.94\tP(random action) = 0.10\n",
      "Ep. 3400)  Time: 232.86 sec\tSteps: 170000\t Avg. reward: 8.19\tP(random action) = 0.10\n",
      "Ep. 3500)  Time: 240.29 sec\tSteps: 175000\t Avg. reward: 7.87\tP(random action) = 0.10\n",
      "Ep. 3600)  Time: 232.42 sec\tSteps: 180000\t Avg. reward: 8.82\tP(random action) = 0.10\n",
      "Ep. 3700)  Time: 232.95 sec\tSteps: 185000\t Avg. reward: 8.91\tP(random action) = 0.10\n",
      "Ep. 3800)  Time: 237.98 sec\tSteps: 190000\t Avg. reward: 8.03\tP(random action) = 0.10\n",
      "Ep. 3900)  Time: 229.39 sec\tSteps: 195000\t Avg. reward: 8.25\tP(random action) = 0.10\n",
      "Ep. 4000)  Time: 235.98 sec\tSteps: 200000\t Avg. reward: 8.68\tP(random action) = 0.10\n",
      "Saved model checkpoint\n",
      "Ep. 4100)  Time: 229.11 sec\tSteps: 205000\t Avg. reward: 8.26\tP(random action) = 0.10\n",
      "Ep. 4200)  Time: 229.62 sec\tSteps: 210000\t Avg. reward: 8.94\tP(random action) = 0.10\n",
      "Ep. 4300)  Time: 234.56 sec\tSteps: 215000\t Avg. reward: 8.53\tP(random action) = 0.10\n",
      "Ep. 4400)  Time: 227.86 sec\tSteps: 220000\t Avg. reward: 8.41\tP(random action) = 0.10\n",
      "Ep. 4500)  Time: 231.12 sec\tSteps: 225000\t Avg. reward: 8.52\tP(random action) = 0.10\n",
      "Ep. 4600)  Time: 229.52 sec\tSteps: 230000\t Avg. reward: 8.05\tP(random action) = 0.10\n",
      "Ep. 4700)  Time: 227.07 sec\tSteps: 235000\t Avg. reward: 8.43\tP(random action) = 0.10\n",
      "Ep. 4800)  Time: 233.65 sec\tSteps: 240000\t Avg. reward: 8.04\tP(random action) = 0.10\n",
      "Ep. 4900)  Time: 227.01 sec\tSteps: 245000\t Avg. reward: 7.54\tP(random action) = 0.10\n",
      "Ep. 5000)  Time: 228.90 sec\tSteps: 250000\t Avg. reward: 8.26\tP(random action) = 0.10\n",
      "Saved model checkpoint\n",
      "Ep. 5100)  Time: 230.95 sec\tSteps: 255000\t Avg. reward: 8.62\tP(random action) = 0.10\n",
      "Ep. 5200)  Time: 224.44 sec\tSteps: 260000\t Avg. reward: 8.23\tP(random action) = 0.10\n",
      "Ep. 5300)  Time: 226.92 sec\tSteps: 265000\t Avg. reward: 7.86\tP(random action) = 0.10\n",
      "Ep. 5400)  Time: 229.00 sec\tSteps: 270000\t Avg. reward: 8.62\tP(random action) = 0.10\n",
      "Ep. 5500)  Time: 225.45 sec\tSteps: 275000\t Avg. reward: 8.16\tP(random action) = 0.10\n",
      "Ep. 5600)  Time: 230.29 sec\tSteps: 280000\t Avg. reward: 7.53\tP(random action) = 0.10\n",
      "Ep. 5700)  Time: 223.02 sec\tSteps: 285000\t Avg. reward: 8.40\tP(random action) = 0.10\n",
      "Ep. 5800)  Time: 227.18 sec\tSteps: 290000\t Avg. reward: 8.09\tP(random action) = 0.10\n",
      "Ep. 5900)  Time: 226.63 sec\tSteps: 295000\t Avg. reward: 8.32\tP(random action) = 0.10\n",
      "Ep. 6000)  Time: 222.87 sec\tSteps: 300000\t Avg. reward: 7.92\tP(random action) = 0.10\n",
      "Saved model checkpoint\n",
      "Ep. 6100)  Time: 227.17 sec\tSteps: 305000\t Avg. reward: 8.34\tP(random action) = 0.10\n",
      "Ep. 6200)  Time: 222.70 sec\tSteps: 310000\t Avg. reward: 8.01\tP(random action) = 0.10\n",
      "Ep. 6300)  Time: 220.84 sec\tSteps: 315000\t Avg. reward: 7.34\tP(random action) = 0.10\n",
      "Ep. 6400)  Time: 226.85 sec\tSteps: 320000\t Avg. reward: 8.42\tP(random action) = 0.10\n",
      "Ep. 6500)  Time: 221.56 sec\tSteps: 325000\t Avg. reward: 8.22\tP(random action) = 0.10\n",
      "Ep. 6600)  Time: 222.03 sec\tSteps: 330000\t Avg. reward: 7.86\tP(random action) = 0.10\n",
      "Ep. 6700)  Time: 224.03 sec\tSteps: 335000\t Avg. reward: 8.03\tP(random action) = 0.10\n",
      "Ep. 6800)  Time: 219.43 sec\tSteps: 340000\t Avg. reward: 8.17\tP(random action) = 0.10\n",
      "Ep. 6900)  Time: 224.50 sec\tSteps: 345000\t Avg. reward: 8.49\tP(random action) = 0.10\n",
      "Ep. 7000)  Time: 219.60 sec\tSteps: 350000\t Avg. reward: 7.52\tP(random action) = 0.10\n",
      "Saved model checkpoint\n",
      "Ep. 7100)  Time: 219.71 sec\tSteps: 355000\t Avg. reward: 8.45\tP(random action) = 0.10\n",
      "Ep. 7200)  Time: 225.87 sec\tSteps: 360000\t Avg. reward: 7.56\tP(random action) = 0.10\n",
      "Ep. 7300)  Time: 219.13 sec\tSteps: 365000\t Avg. reward: 8.14\tP(random action) = 0.10\n",
      "Ep. 7400)  Time: 219.50 sec\tSteps: 370000\t Avg. reward: 7.99\tP(random action) = 0.10\n",
      "Ep. 7500)  Time: 223.02 sec\tSteps: 375000\t Avg. reward: 8.18\tP(random action) = 0.10\n",
      "Ep. 7600)  Time: 217.99 sec\tSteps: 380000\t Avg. reward: 8.35\tP(random action) = 0.10\n",
      "Ep. 7700)  Time: 221.50 sec\tSteps: 385000\t Avg. reward: 8.93\tP(random action) = 0.10\n",
      "Ep. 7800)  Time: 221.08 sec\tSteps: 390000\t Avg. reward: 7.63\tP(random action) = 0.10\n",
      "Ep. 7900)  Time: 217.86 sec\tSteps: 395000\t Avg. reward: 8.11\tP(random action) = 0.10\n",
      "Ep. 8000)  Time: 223.71 sec\tSteps: 400000\t Avg. reward: 8.20\tP(random action) = 0.10\n",
      "Saved model checkpoint\n",
      "Ep. 8100)  Time: 218.97 sec\tSteps: 405000\t Avg. reward: 7.85\tP(random action) = 0.10\n",
      "Ep. 8200)  Time: 217.47 sec\tSteps: 410000\t Avg. reward: 8.53\tP(random action) = 0.10\n",
      "Ep. 8300)  Time: 223.50 sec\tSteps: 415000\t Avg. reward: 8.28\tP(random action) = 0.10\n",
      "Ep. 8400)  Time: 217.31 sec\tSteps: 420000\t Avg. reward: 7.70\tP(random action) = 0.10\n",
      "Ep. 8500)  Time: 218.53 sec\tSteps: 425000\t Avg. reward: 8.32\tP(random action) = 0.10\n",
      "Ep. 8600)  Time: 221.99 sec\tSteps: 430000\t Avg. reward: 8.16\tP(random action) = 0.10\n",
      "Ep. 8700)  Time: 216.10 sec\tSteps: 435000\t Avg. reward: 8.95\tP(random action) = 0.10\n",
      "Ep. 8800)  Time: 221.81 sec\tSteps: 440000\t Avg. reward: 8.13\tP(random action) = 0.10\n",
      "Ep. 8900)  Time: 218.52 sec\tSteps: 445000\t Avg. reward: 7.90\tP(random action) = 0.10\n",
      "Ep. 9000)  Time: 229.36 sec\tSteps: 450000\t Avg. reward: 8.04\tP(random action) = 0.10\n",
      "Saved model checkpoint\n",
      "Ep. 9100)  Time: 224.01 sec\tSteps: 455000\t Avg. reward: 8.18\tP(random action) = 0.10\n",
      "Ep. 9200)  Time: 221.23 sec\tSteps: 460000\t Avg. reward: 8.46\tP(random action) = 0.10\n",
      "Ep. 9300)  Time: 224.86 sec\tSteps: 465000\t Avg. reward: 8.30\tP(random action) = 0.10\n",
      "Ep. 9400)  Time: 226.61 sec\tSteps: 470000\t Avg. reward: 7.87\tP(random action) = 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 9500)  Time: 225.02 sec\tSteps: 475000\t Avg. reward: 7.64\tP(random action) = 0.10\n",
      "Ep. 9600)  Time: 222.57 sec\tSteps: 480000\t Avg. reward: 8.32\tP(random action) = 0.10\n",
      "Ep. 9700)  Time: 217.86 sec\tSteps: 485000\t Avg. reward: 7.26\tP(random action) = 0.10\n",
      "Ep. 9800)  Time: 216.86 sec\tSteps: 490000\t Avg. reward: 8.57\tP(random action) = 0.10\n",
      "Ep. 9900)  Time: 223.41 sec\tSteps: 495000\t Avg. reward: 8.14\tP(random action) = 0.10\n",
      "Ep. 10000)  Time: 217.96 sec\tSteps: 500000\t Avg. reward: 7.63\tP(random action) = 0.10\n",
      "Percent of successful episodes: 7.981%\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(h_size, lr = lr)\n",
    "targetQN = QNetwork(h_size, lr = lr)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Add the ops to update the targetQN parameters to the TensorFlow graph\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables, tau)\n",
    "\n",
    "# Initialize the experience replay buffer\n",
    "myBuffer = experience_buffer(buffer_size = buffer_size)\n",
    "\n",
    "# Set the rate decrease for the probability of a random action\n",
    "e = startE\n",
    "stepDrop = (startE - endE) / annealing_steps\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "# Saved model directory\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    if load_model:\n",
    "        print('Loading model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "    t_start = time.time()\n",
    "    # Main loop\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        # Reset environment and get first new observation\n",
    "        s = processState(env.reset(), img_sz)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        # Training loop\n",
    "        while j < max_epLength:\n",
    "            j += 1\n",
    "            # Choose an action by greedily (with e chance of random action) from Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0, 4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict, feed_dict = {mainQN.scalarInput: [s]})[0]\n",
    "            s1, r, d = env.step(a)\n",
    "            s1 = processState(s1, img_sz)\n",
    "            total_steps += 1\n",
    "            # Add this experience to the replay buffer\n",
    "            episodeBuffer.add(np.reshape(np.array([s, a, r, s1, d]), [1, 5]))\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                    \n",
    "                if total_steps % update_freq == 0:\n",
    "                    # Train the models on the buffered experiences\n",
    "                    # Get the batch of experiences from the buffer\n",
    "                    trainBatch = myBuffer.sample(batch_size)\n",
    "                    # Run the two networks in feed forward. The main network\n",
    "                    # will output an action to take\n",
    "                    Q1 = sess.run(mainQN.predict, feed_dict = {          # Target actions\n",
    "                        mainQN.scalarInput: np.vstack(trainBatch[:,3]),\n",
    "                    })\n",
    "                    Q2 = sess.run(targetQN.Qout, feed_dict = {           # Target Q-values\n",
    "                        targetQN.scalarInput: np.vstack(trainBatch[:,3]),\n",
    "                    })\n",
    "                    # Zero out the experiences that result in a finished episode\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    # Get the target Q-values of JUST the target actions from mainQN\n",
    "                    doubleQ = Q2[range(batch_size), Q1]\n",
    "                    # targetQ = r + gamma*Q(targetAction)\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    # Update mainDQN with our target values\n",
    "                    sess.run(mainQN.updateModel, feed_dict = {\n",
    "                        mainQN.scalarInput: np.vstack(trainBatch[:,0]),\n",
    "                        mainQN.targetQ    : targetQ,\n",
    "                        mainQN.actions    : trainBatch[:,1],\n",
    "                    })\n",
    "                    # Update targetDQN's parameters towards mainDQN's\n",
    "                    updateTarget(targetOps, sess)\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d:\n",
    "                break\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        # Periodically save the model\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            saver.save(sess, os.path.join(save_dir, 'model-'+str(i)+'.ckpt'))\n",
    "            print('Saved model checkpoint')\n",
    "        if len(rList) % log_time == 0 and len(rList) != 0:\n",
    "            t_elapsed = time.time() - t_start\n",
    "            print('Ep. {:5d})  Time: {:.2f} sec\\tSteps: {:6d}\\tAvg. reward: {:.2f}\\tP(random action) = {:.2f}'.format(\n",
    "                i+1, t_elapsed, total_steps, np.mean(rList[-log_time:]), e\n",
    "            ))\n",
    "            t_start = time.time()\n",
    "    saver.save(sess, os.path.join(save_dir, 'model-'+str(i)+'.ckpt'))\n",
    "print(\"Percent of successful episodes: %.3f%%\" % (sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check network learning\n",
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25384ebb748>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x253bf8c4550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList), [len(rList)//100, 100])\n",
    "rMean = np.average(rMat, 1)\n",
    "plt.plot(rMean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
